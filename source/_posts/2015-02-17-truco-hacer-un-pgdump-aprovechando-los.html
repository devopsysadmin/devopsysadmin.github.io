---
layout: post
title: Hacer un pg_dump aprovechando los cores de la máquina
date: '2015-02-17T07:06:00.000-08:00'
author: David PG
tags:
- linux
- trucos
- postgresql
modified_time: '2015-02-18T08:35:15.824-08:00'
blogger_id: tag:blogger.com,1999:blog-4659522467025728448.post-8416402531873167492
blogger_orig_url: http://devopsysadmin.blogspot.com/2015/02/truco-hacer-un-pgdump-aprovechando-los.html
---

<p>Desde la versión 9.3, postgreSQL permite hacer dumps de la base de datos en forma paralela, aprovechando así el número de cores de la máquina.</p>
<p>En una forma rápida, podemos lanzar este proceso:</p>
<p><pre>pg_dump -h $MAQUINA -U $USER -d $DATABASE -Fd -j `nproc` -f $DESTINO</pre></p>
<p>Las variables son autorreferentes, con lo que la gracia de este comando se encuentra en la opción -Fd (obligatoria para que use diferentes cores), que realiza el dump en un directorio en lugar de un archivo; y la opción -j, que refiere al número de procesos que se lanzan simultáneamente. En este caso, hacemos uso de lo que devuelve <code>nproc</code> para levantar tantos procesos como núcleos.</p>
<p>Un pequeño script de backup de base de datos local (perfecto para meter en un cronjob del usuario postgres) sería:</p>

{% highlight bash %}
#!/bin/bash
## This script will create a backup for each database listed in the DATABASES variable
## Use as postgres user or add the -U [username] to the pg_dump line

DATABASES="template1 db other_db"
CORES=`nproc`
TARGET_FOLDER=/var/backups

for DATABASE in $DATABASES; do
	mkdir -p $TARGET_FOLDER/$DATABASE
	pg_dump -d $DATABASE -Fd -j $CORES -f $TARGET_FOLDER/$DATABASE
done
{% endhighlight %}